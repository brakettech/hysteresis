{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set(style='darkgrid')\n",
    "from pipe import Pipe\n",
    "import easier as ezr\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "import holoviews as hv\n",
    "# hv.extension('bokeh')\n",
    "\n",
    "pd.set_option(\"display.max_columns\",101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the directory holding data files\n",
    "data_dir = './20180318/'\n",
    "\n",
    "# the file name for the test log (it should live in the data directory)\n",
    "test_log_file_base_name = 'test_log_20180318.csv'\n",
    "\n",
    "# this is the maximum number of identical samples to take \n",
    "max_samples = 3\n",
    "channel_mapper= dict(\n",
    "    a='sig_gen',\n",
    "    b='res_volt',\n",
    "    c='rec_volt',\n",
    "    d='sec_volt'\n",
    ")\n",
    "\n",
    "# compute the full path of the test log file\n",
    "test_log_file = os.path.join(data_dir, test_log_file_base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_files(data_dir):\n",
    "    \"\"\"\n",
    "    Find all data files under a specified directory\n",
    "    \"\"\"\n",
    "    # the regex pattern for identifying a data file\n",
    "    rex_file = re.compile(r'.*/?\\d+\\-\\d+(_\\d+)?\\.csv')\n",
    "    \n",
    "    # initialize empty list of data files\n",
    "    data_files = []\n",
    "    \n",
    "    # recursively search data directory\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            # only consider data files\n",
    "            if rex_file.match(file):\n",
    "                \n",
    "                # compute the full path to the datafile\n",
    "                file_name = os.path.join(root, file)\n",
    "                \n",
    "                # the file_tag is what is put into the log file\n",
    "                file_tag = re.sub(r'(_\\d+)?.csv', '', file)\n",
    "                \n",
    "                # add the data file\n",
    "                data_files.append((file_tag, file_name))\n",
    "    \n",
    "    # create and return the output dataframe\n",
    "    df_files = pd.DataFrame(data_files, columns=['file_tag', 'file_name'])\n",
    "    return df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_log_frame(test_log_file, data_dir, max_samples):\n",
    "    # read in the test log\n",
    "    df_log = pd.read_csv(test_log_file).drop(['test_no'], axis=1)\n",
    "    \n",
    "    # standardize column names\n",
    "    df_log = df_log.rename(\n",
    "        columns=dict(primary_position='pos', file_name='file_tag', fatigue_life='life'))\n",
    "    \n",
    "    # sometimes the file will have blank fields.  These are garbage\n",
    "    df_log.dropna(inplace=True)\n",
    "    \n",
    "    # get a frame of all files in the data directory\n",
    "    df_files = find_files(data_dir)\n",
    "    \n",
    "    # Use the file_tag to link each data file with its corresponding log-file entry\n",
    "    df_out = pd.merge(df_log, df_files, on=['file_tag'], how='right')\n",
    "    \n",
    "    \n",
    "    def compute_sample_num(batch):\n",
    "        \"\"\"\n",
    "        For each condition measured, this function computes the sample numbe\n",
    "        \"\"\"\n",
    "        batch.insert(3, 'sample_num', np.array(range(len(batch))) + 1)\n",
    "        return batch  \n",
    "    \n",
    "    # These fields identify measurement conditions (that can be sample multiple times)\n",
    "    grouping_fields = ['sample', 'frequency', 'bends', 'pos']\n",
    "    \n",
    "    # This will order by filename within batches, although it's no clear that's needed\n",
    "    sorting_fields = grouping_fields + ['file_name']\n",
    "    df_out = df_out.sort_values(by=sorting_fields)\n",
    "    \n",
    "    # Actually run the groupby to assign sample number\n",
    "    df_out = df_out.groupby(by=grouping_fields).apply(compute_sample_num)\n",
    "    \n",
    "    # Select the final output fields you want in the log frame\n",
    "    df_out = df_out[grouping_fields + ['sample_num', 'file_name']]\n",
    "    \n",
    "    # Don't include more than max_samples for each measurement condition\n",
    "    df_out = df_out[df_out.sample_num <= max_samples].reset_index(drop=True)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "df_log = make_log_frame(test_log_file, data_dir, max_samples=max_samples)\n",
    "display(df_log.head(5))\n",
    "print(len(df_log))\n",
    "print('sample', sorted(df_log['sample'].unique()))\n",
    "print('freq', sorted(df_log.frequency.unique()))\n",
    "print('bends', sorted(df_log.bends.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_data(df_log, data_dir, channel_mapper, njobs=1, recompute=False):\n",
    "    \"\"\"\n",
    "    Run the computation across files to extract features\n",
    "    \"\"\"\n",
    "    # Define the file (in the data directory) that will hold the analysis results\n",
    "    results_file = os.path.join(data_dir, 'results.txt')\n",
    "    \n",
    "    # These computations can take a really long time, so only recompute if you have to\n",
    "    if recompute:\n",
    "        \n",
    "        # We will be appending a bunch of frames, so initialze to no frame\n",
    "        p = Pipe(df_log, channel_mapper, n_jobs=njobs, harmonic=3)\n",
    "        p.process()\n",
    "        df = p.df\n",
    "\n",
    "        df.to_csv(results_file, index=False)\n",
    "        \n",
    "    df = pd.read_csv(results_file)\n",
    "    return df\n",
    "\n",
    "with ezr.Timer('get_data'):\n",
    "    df = get_data(df_log, data_dir, channel_mapper, njobs=2, recompute=False)\n",
    "    \n",
    "####################3\n",
    "#TODO: Bake this into the log frame\n",
    "df.rename(columns=dict(sample='sample_name'), inplace=True)\n",
    "df.loc[:, 'sample_name'] = df.sample_name.str.replace('_.*', '')\n",
    "df.head()\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_experiment(df, xparams, col_name):\n",
    "    pl.rcParams['figure.figsize'] = (20, 6)\n",
    "    sns.set_context('talk')\n",
    "\n",
    "    # successively filter the frame to have only params listed in xparams\n",
    "    key_cols = list(xparams.keys()) + ['pos']\n",
    "    for k, v in xparams.items():\n",
    "        if isinstance(v, tuple):\n",
    "            overlay_col = k\n",
    "            baseline_val = v[0]\n",
    "        else:\n",
    "            df = df[df[k] == v]\n",
    "            \n",
    "    # pull out the baseline_frames\n",
    "    baseline_batch = df[df[overlay_col] == baseline_val]\n",
    "    baseline_batch = baseline_batch.groupby(by=key_cols).mean().reset_index()\n",
    "    \n",
    "            \n",
    "    # Run plotting overlays for all overlay groups\n",
    "    for key, batch in df.groupby(by=overlay_col):\n",
    "        mean_batch = batch.groupby(by=key_cols).mean().reset_index()\n",
    "        \n",
    "        pl.figure()\n",
    "        _plot_frame(xparams, batch, baseline_batch, mean_batch, col_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_frame(xparams, df, df_baseline, df_mean,  col_name):\n",
    "    title_val = ' '.join([f'{k}={v[0] if isinstance(v, tuple) else v}' for (k, v) in xparams.items()])\n",
    "    baseline_col = f'{col_name}_b'\n",
    "    mean_col = f'{col_name}_m'\n",
    "    \n",
    "    df = df[['pos', col_name]]\n",
    "    dfb = df_baseline[['pos', col_name]]\n",
    "    dfm = df_mean[['pos', col_name]]\n",
    "    \n",
    "    dfj = pd.merge(df, dfb, on='pos', suffixes=['', '_b']).dropna()\n",
    "    dfj = pd.merge(dfj, dfm, on='pos', suffixes=['', '_m']).dropna()\n",
    "    \n",
    "    \n",
    "    dfj.loc[:, 'delta'] = dfj[col_name] - dfj[baseline_col]\n",
    "    dfj.loc[:, 'ratio'] = dfj[col_name] / dfj[baseline_col]\n",
    "    dfj.loc[:, 'delta_m'] = dfj[mean_col] - dfj[baseline_col]\n",
    "    dfj.loc[:, 'ratio_m'] = dfj[mean_col] / dfj[baseline_col]\n",
    "    \n",
    "    pl.subplot(131)\n",
    "    pl.plot(dfj.pos, dfj[baseline_col])\n",
    "    pl.plot(dfj.pos, dfj[mean_col])\n",
    "    pl.plot(dfj.pos, dfj[col_name], 'o')\n",
    "    pl.xlabel('Position along Pipe (inches)')\n",
    "    pl.title(title_val)\n",
    "    pl.ylabel(col_name)\n",
    "    \n",
    "    pl.subplot(132)\n",
    "    pl.plot(dfj.pos, dfj.ratio, 'o')\n",
    "    pl.plot(dfj.pos, dfj.ratio_m, '-')\n",
    "    \n",
    "    pl.title('Ratio')\n",
    "\n",
    "    pl.subplot(133)\n",
    "    pl.plot(dfj.pos, dfj.delta, 'o')\n",
    "    pl.plot(dfj.pos, dfj.delta_m, '-')\n",
    "    \n",
    "    pl.title('Difference')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_log.groupby(by=['sample', 'frequency', 'bends']).count() / max_samples)\n",
    "\n",
    "xparams = dict(\n",
    "    sample_name='ss',\n",
    "    frequency=1000,\n",
    "    bends=(0.,),\n",
    ")\n",
    "\n",
    "# xparams = dict(\n",
    "#     sample_name='ss',\n",
    "#     frequency=(50,),\n",
    "#     bends=0.,\n",
    "# )\n",
    "\n",
    "\n",
    "plot_experiment(df, xparams, 'sec_harm_db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
